---
id: L-107
date: 2024.01.20
category: RESEARCH
title: 'Attention Is All You Need — Paper Breakdown'
readTime: 15 MIN
preview: 'A personal deep dive into the Transformer architecture paper, with annotated diagrams and implementation notes.'
tags:
  - ai
  - research
  - transformers
---
# Attention Is All You Need — Paper Breakdown

## Why This Paper Matters

Published in 2017, this paper introduced the **Transformer architecture** that now powers GPT, BERT, and virtually every modern LLM.

## Key Insight: Self-Attention

The core innovation is replacing recurrence with **multi-head self-attention**:

```
Attention(Q, K, V) = softmax(QK^T / √d_k) · V
```

Where:
- **Q** (Query): What am I looking for?
- **K** (Key): What do I contain?
- **V** (Value): What do I actually output?

## Multi-Head Attention

Instead of single attention, run **h parallel heads**:

```python
def multi_head_attention(Q, K, V, num_heads=8):
    d_model = Q.shape[-1]
    d_k = d_model // num_heads

    heads = []
    for i in range(num_heads):
        Q_i = Q @ W_q[i]  # Project to subspace
        K_i = K @ W_k[i]
        V_i = V @ W_v[i]
        heads.append(attention(Q_i, K_i, V_i))

    return concat(heads) @ W_o
```

## Positional Encoding

Since Transformers have no inherent notion of sequence order, positional information is injected via sinusoidal encoding:

```
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

> My takeaway: the brilliance is in *removing* complexity (no recurrence, no convolution) rather than adding it.
